{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Dive into the S&P 500: Predicting Stock Prices\n",
    "Kanishk Chinnapapannagari, Aarav Naveen, Avyay Potarlanka, and Melvin Rajendran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today’s evolving financial landscape, both investors and traders are constantly seeking an edge to make informed decisions. The stock market, which contains an intricate web of variables and is influenced by numerous factors, has proven to be a difficult environment to navigate.\n",
    "\n",
    "In the past, investment-related decisions were often made based on analysis of historical trends. However, the advancement of data science and machine learning techniques has introduced a new opportunity to potentially predict future stock prices with reasonable accuracy and thus gain valuable insights.\n",
    "\n",
    "This data science project delves into prediction of stock prices within the Standard & Poor’s 500 index, otherwise known as the S&P 500. This index contains 500 of the top companies in the United States, and it represents approximately 80% of the U.S. stock market’s total value. Hence, it serves as a strong indicator of the movement within the market. To learn more about the S&P 500 and other popular indices in the U.S., read this article: https://www.investopedia.com/insights/introduction-to-stock-market-indices/.\n",
    "\n",
    "Throughout this project, we will follow a comprehensive data science approach that includes the following steps:\n",
    "* Data collection\n",
    "* Data processing\n",
    "* Exploratory data analysis and data visualization\n",
    "* Data analysis, hypothesis testing, and machine learning (ML)\n",
    "* Insight formation\n",
    "\n",
    "Our project aims to leverage predictive modeling techniques to provide insights to investors. The analysis herein will identify stocks that are undervalued and thus will increase in price in the near future, meaning investors should consider buying or holding shares. Likewise, it will also identify stocks that are overvalued and will soon decrease in price, indicating that investors should consider selling their position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in a Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather information about the S&P 500 companies, we will be using the following dataset: https://www.kaggle.com/datasets/paultimothymooney/stock-market-data. This Kaggle dataset contains the date, volume, and prices for the NASDAQ, NYSE, and S&P 500. For the purposes of this project, we will only analyze the stock prices of companies in the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty data frame to store the stock price data\n",
    "price_data = pd.DataFrame()\n",
    "\n",
    "# Initialize the path to the folder containing the data\n",
    "folder_path = 'sp500-data'\n",
    "\n",
    "# Iterate across each file in the folder by name\n",
    "for file_name in os.listdir(folder_path):\n",
    "    \n",
    "    # Check if the current file is a CSV file\n",
    "    if file_name.endswith('.csv'):\n",
    "        \n",
    "        # Read the current file into a temporary data frame\n",
    "        temp = pd.read_csv(os.path.join(folder_path, file_name))\n",
    "        \n",
    "        # Extract the symbol from the current file's name\n",
    "        symbol = file_name[0:-4]\n",
    "        \n",
    "        # Store the symbol in a new column in the temporary data frame \n",
    "        temp['Symbol'] = symbol\n",
    "        \n",
    "        # Concatenate the accumulating and temporary data frames\n",
    "        price_data = pd.concat([price_data, temp], ignore_index = True)\n",
    "\n",
    "# Print the last five rows of the price data frame\n",
    "price_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping From Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the Kaggle dataset does not contain sector data. For this reason, we will supplement our existing data with that which is contained on the following webpage: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies. By scraping this webpage's list of the S&P 500 companies, we can match each company in our existing data to its corresponding GICS sector and sub-industry. This will enable us to perform analysis by sector and/or sub-industry and thus eliminate biases in our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for the HTTP request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'From': 'pleaseletmein@gmail.com'\n",
    "}\n",
    "\n",
    "# Make an HTTP request to the Wikipedia URL and store the response\n",
    "response = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies', headers = headers)\n",
    "\n",
    "# Parse the text from the webpage as HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table element containing the data and both extract and store the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Read the HTML table into a data frame\n",
    "sector_data = pd.read_html(str(table), flavor = 'html5lib')[0]\n",
    "\n",
    "# Print the last five rows of the sector data frame\n",
    "sector_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping From Slickcharts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to focus our attention on the top companies of each sector, as these companies drive the movement within their respective sectors. Hence, we will scrape the data from the following webpage: https://www.slickcharts.com/sp500. This webpage contains a list of the S&P 500 companies by weight, where weight is equal to a company's market cap divided by the overall value of the S&P 500. Ultimately, we will select the top companies of each sector by weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an HTTP request to the Slickcharts URL and store the response\n",
    "response = requests.get('https://www.slickcharts.com/sp500', headers = headers)\n",
    "\n",
    "# Parse the text from the webpage as HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table element containing the data and both extract and store the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Read the HTML table into a data frame\n",
    "weight_data = pd.read_html(str(table), flavor = 'html5lib')[0]\n",
    "\n",
    "# Print the last five rows of the sector data frame\n",
    "weight_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have three data frames containing data that was collected in the previous step. We will merge this data into a single data frame. Then, we will filter our data to include only the top five companies within each sector. As part of this process, we need to clean our data. Data cleaning will involve casting our data to the proper types, removing entries with missing values, and removing unnecessary columns.\n",
    "\n",
    "### Cleaning the Sector Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the sector and industry-related columns\n",
    "sector_data = sector_data.rename(columns = {'GICS Sector': 'Sector', 'GICS Sub-Industry': 'Industry'})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "sector_data = sector_data.drop(['Headquarters Location', 'Date added', 'CIK', 'Founded'], axis = 1)\n",
    "\n",
    "# Print the last five rows of the data frame\n",
    "sector_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Weight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except Symbol and Weight\n",
    "weight_data = weight_data.drop(['#', 'Company', 'Price', 'Chg', '% Chg'], axis = 1)\n",
    "\n",
    "# Print the last five rows of the data frame\n",
    "weight_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Three Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an inner join (merge) on all three data frames to create a single data frame\n",
    "data = pd.merge(pd.merge(price_data, sector_data, on = 'Symbol'), weight_data, on = 'Symbol')\n",
    "\n",
    "# Reindex the columns of the data frame\n",
    "data = data.reindex(columns = ['Symbol', 'Security', 'Sector', 'Industry', 'Weight', 'Date', 'Open', 'High', 'Low', 'Close', 'Adjusted Close', 'Volume'])\n",
    "\n",
    "# Cast the Date column's type to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst = True)\n",
    "\n",
    "# Print the last five rows of the resulting data frame\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Top 5 Companies Within Each Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty data frame to contain the filtered data\n",
    "top_data = pd.DataFrame()\n",
    "\n",
    "# Iterate across a list of the unique sectors\n",
    "for sector in data['Sector'].unique():\n",
    "    \n",
    "    # Filter the data by the current sector\n",
    "    sector_data = data[data['Sector'] == sector]\n",
    "\n",
    "    # Compile a list of the top five weights in the current sector\n",
    "    top_five_weights = sorted(sector_data['Weight'].unique(), reverse = True)[:5]\n",
    "    \n",
    "    # Filter the data by the top five weights\n",
    "    sector_data = sector_data[sector_data['Weight'].isin(top_five_weights)]\n",
    "    \n",
    "    # Concatenate the top five companies' data into the accumulating dataframe\n",
    "    top_data = pd.concat([top_data, sector_data], ignore_index = True)\n",
    "\n",
    "# Print the last five rows of the filtered data frame\n",
    "top_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit a machine learning model to our data, we would like to visualize it by sector and preliminarily determine relationships between the data. In particular, we would like to analyze how strongly the stock prices of companies within the same sector are correlated.\n",
    "\n",
    "For the remainder of our analysis, we will focus our attention on adjusted close price, which is explained in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Adjusted Close Price vs. Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted close price is the final price at which a security  trades at the end of a trading day, adjusting for dividends, stock splits, and new offerings. It is the most accurate representation of a company's stock price, and it is commonly used by investors and traders to track performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a plot for the top five companies in each sector\n",
    "for sector in top_data['Sector'].unique():\n",
    "    \n",
    "    # Filter the data for the current sector\n",
    "    sector_data = top_data[top_data['Sector'] == sector]\n",
    "    \n",
    "    # Reshape the data for plotting purposes\n",
    "    sec_as_row = sector_data.pivot(index = 'Date', columns = 'Symbol', values = 'Adjusted Close')\n",
    "    \n",
    "    # Generate plot\n",
    "    sec_as_row.plot(title = f'{sector}: Adjusted Close Price vs. Date', legend = True, xlabel = 'Date', ylabel = 'Adjusted Close Price', figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are 11 line plots of adjusted close price vs. date for the the top five companies (by weight) in each sector.\n",
    "\n",
    "In the Health Care sector, one company had a much higher close price while the other four were closely correlated with one another. This case of the top comapny having a significantly greater closing price while the other four were much lower but closer to each other is a general trend that is visible through several of these graphs. In addition to the Health Care sector, this trend is present in the Financials sector, Consumer Staples sector, and Industrials sector, but interestingly in the Industrials sector as the top company's adjusted close price began to fall, the other four companies' adjusted close price rose together rather than one company taking over and continuing the trend. Other sectors have closer adjusted close prices amongst the top 5 companies: for example, in the Information Technology sector, ACN, MSFT, and CRM follow similar growth trends and maintain a similar price over the years while CSCO and AAPL trail behind. Also, in the Energy sector, MPC, XOM, COP, and EOG, essentially follow the same trend and stock price while SLB is consistently lower, so within this sector four companies are equally competitive rather than the trend of one company dominance that was seen in other sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Volume vs. Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume traded is the number of shares that are transferred between constituents during the trading day. This is an important metric for investors and traders to consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a plot for the top five companies in each sector\n",
    "for sector in top_data['Sector'].unique():\n",
    "    \n",
    "    # Filter the data for the current sector\n",
    "    sector_data = top_data[top_data['Sector'] == sector]\n",
    "    \n",
    "    # Reshape the data for plotting purposes\n",
    "    sec_as_row = sector_data.pivot(index = 'Date', columns = 'Symbol', values = 'Volume')\n",
    "    \n",
    "    # Generate plot\n",
    "    sec_as_row.plot(title = f'{sector}: Volume vs. Date', legend = True, xlabel = 'Date', ylabel = 'Volume', figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are 11 line plots of volume traded vs. date for the the top five companies (by weight) in each sector.\n",
    "\n",
    "It is apparent that while there are correlations amongst the companies within each sector, one company often dominates the volume traded or has strong, isolated shifts. For example, in the Consumer Discretionary sector, Amazon (ticker AMZN) has the greatest volume traded since approximately 2000. It has had up to 2 billion dollar trading volumes at certain points. Similarly, in the Financials sector, Bank of America (ticker BAC) has the greatest volume traded since approximately 2010. It has had up to 1 billion dollar trading volumes at certain points, whereas its competitors have only had up to 600 million dollar trading volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Various Moving Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving average standardizes the price of a stock by converting it to a constantly updated average price. This average is calculated over a predetermined time period. The most relevant and commonly used time periods for calculating moving average are 10 days and 20 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lengths of moving averages (in days) to calculate\n",
    "moving_averages = [10, 20]\n",
    "\n",
    "# Iterate across the moving averages\n",
    "for ma in moving_averages:\n",
    "\n",
    "    # Iterate across each company\n",
    "    for security in top_data['Security'].unique():\n",
    "        \n",
    "        # Filter the data for the current company\n",
    "        security_data = top_data[top_data['Security'] == security]\n",
    "        \n",
    "        # Add a column containing the current company's moving average\n",
    "        top_data[f'{ma}-Day Moving Average'] = top_data['Adjusted Close'].rolling(ma).mean()\n",
    "    \n",
    "# Print the last five rows of the data frame\n",
    "top_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Daily Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily return is the percentage change in the price of stock over the course of a trading day. This will help us assess the risk of investing in a particular company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty data frame to contain the daily return values\n",
    "return_data = pd.DataFrame()\n",
    "\n",
    "# Iterate across the sectors\n",
    "for security in top_data['Security'].unique():\n",
    "    \n",
    "    # Filter the data for the current sector\n",
    "    security_data = top_data[top_data['Security'] == security]\n",
    "    \n",
    "    # Calculate the percent change i.e. daily return\n",
    "    security_rets = pd.DataFrame(security_data['Adjusted Close'].pct_change())\n",
    "\n",
    "    # Append this data to the accumulating data frame\n",
    "    return_data = pd.concat([return_data, security_rets], ignore_index = True)\n",
    "\n",
    "# Add the daily return values to the top company data frame\n",
    "top_data['Daily Return'] = return_data\n",
    "\n",
    "# Print the last five rows of the top data frame\n",
    "top_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Comparing the Daily Returns of Various Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the daily returns of various stocks against one another. This will help us assess whether the stock prices of companies in the same sector are strongly correlated or not. We expect that they are linearly and positively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a data frame to contain the formatted data for plotting\n",
    "formatted_data = top_data[['Symbol', 'Date', 'Daily Return']]\n",
    "\n",
    "# Pivots the ticker symbols from a column's entries to column headers\n",
    "formatted_data = formatted_data.pivot(index = 'Date', columns = 'Symbol', values = 'Daily Return')\n",
    "\n",
    "# Print the last five rows of the formatted data frame\n",
    "formatted_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis, Hypothesis Testing, and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE OUT THE 5 X 5 for the 11 sectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract top company by market cap in each category. Due to computer limitations this is a necessary action as training a machine learning model is incredibly time consuming and resource intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty data frame to contain the filtered data\n",
    "topmost_data = pd.DataFrame()\n",
    "\n",
    "# Iterate across a list of the unique sectors\n",
    "for sector in top_data['Sector'].unique():\n",
    "    \n",
    "    # Filter the data by the current sector\n",
    "    sector_data = top_data[top_data['Sector'] == sector]\n",
    "\n",
    "    # Get top weight in the current sector\n",
    "    top_weight = sorted(sector_data['Weight'].unique(), reverse = True)[:1]\n",
    "    \n",
    "    # Get company with the calculated top market cap\n",
    "    sector_data = sector_data[sector_data['Weight'].isin(top_weight)]\n",
    "    \n",
    "    # Concatenate the company's data into the accumulating dataframe\n",
    "    topmost_data = pd.concat([topmost_data, sector_data], ignore_index = True)\n",
    "\n",
    "# Reassign top data\n",
    "top_data = topmost_data\n",
    "\n",
    "# Print the last five rows of the filtered data frame\n",
    "top_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get necessary columns\n",
    "formatted_data = top_data[['Symbol', 'Date', 'Daily Return']]\n",
    "\n",
    "# Pivot to change format such that symbols are columns\n",
    "formatted_data = formatted_data.pivot(index='Date', columns='Symbol', values='Daily Return')\n",
    "\n",
    "# Print last 5 rows\n",
    "formatted_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_data = top_data.filter(['Close','Symbol'])\n",
    "close_data = close_data[close_data['Symbol'] == 'AAPL']\n",
    "close_data = close_data.drop(columns=['Symbol'])\n",
    "\n",
    "pre_train = close_data.values\n",
    "\n",
    "close_data.head()\n",
    "\n",
    "training_data_len = int(np.ceil( len(pre_train) * .95 ))\n",
    "\n",
    "training_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(pre_train)\n",
    "\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scaled_data[0:int(training_data_len), :]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "    if i<= 61:\n",
    "        print(x_train)\n",
    "        print(y_train)\n",
    "        print()\n",
    "        \n",
    "# Convert the x_train and y_train to numpy arrays \n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the testing data set\n",
    "# Create a new array containing scaled values from index 1543 to 2002 \n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "# Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = pre_train[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i-60:i, 0])\n",
    "    \n",
    "# Convert the data to a numpy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Reshape the data\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "# Get the models predicted price values \n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Get the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "train = top_data[top_data['Symbol'] == 'AAPL'][:training_data_len]\n",
    "valid = top_data[top_data['Symbol'] == 'AAPL'][training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
